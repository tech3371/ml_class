{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the csv dataset as a pandas dataframe for ease of manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get feature names\n",
    "names = pd.read_table('train.names', delimiter = ':', names = ['features','options'])\n",
    "feature_names = np.asarray(names['features'])\n",
    "feature_options = np.asarray(names['options'])\n",
    "\n",
    "#get feature data\n",
    "train_x = pd.read_table('train-features.csv', delimiter = ',', names = feature_names[:-1])\n",
    "#get label data (income bool)\n",
    "train_y = np.genfromtxt('train-output.csv')\n",
    "\n",
    "#get test feature data \n",
    "test_x = pd.read_table('test-features.csv', delimiter = ',', names = feature_names[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical features with one hot encoding for numerical classification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get non continuous options\n",
    "mask_continuous = np.asarray(names['options']) == ' continuous.'\n",
    "noncon_feat = feature_names[np.logical_not(mask_continuous)][:-1] #non continuous feature names\n",
    "\n",
    "#iterate across all categorical features and one hot encode them\n",
    "for feat in noncon_feat:\n",
    "    #get the different options for this feature\n",
    "    ind = np.where(feature_names == feat)[0][0]\n",
    "#     options = feature_options[ind].split(',')\n",
    "#     options = [option.strip() for option in options]\n",
    "#     print(options)\n",
    "\n",
    "    #train\n",
    "    add = pd.get_dummies(train_x[feat], prefix = feat)\n",
    "    train_x = train_x.join(add)\n",
    "    \n",
    "    #test \n",
    "    add = pd.get_dummies(test_x[feat], prefix = feat)\n",
    "    test_x = test_x.join(add)\n",
    "\n",
    "#align the two dataframes, so that the test frame has the same shape as the train frame\n",
    "train_x, test_x = train_x.align(test_x, join ='left', axis = 1, fill_value = 0)\n",
    "\n",
    "#drop the unknown work class \n",
    "train_x = train_x.drop('workclass_ ?', axis=1)\n",
    "test_x = test_x.drop('workclass_ ?', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16281, 115)\n",
      "(32561, 115)\n"
     ]
    }
   ],
   "source": [
    "#check the shape and columns of the features\n",
    "print(np.shape(test_x))\n",
    "print(np.shape(train_x))\n",
    "# print(train_x.columns)\n",
    "# print(test_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of original categorical data\n",
    "train_vars = train_x.columns.values.tolist()\n",
    "to_keep = [i for i in train_vars if i not in noncon_feat]\n",
    "train_x_final = train_x[to_keep]\n",
    "\n",
    "test_vars = test_x.columns.values.tolist()\n",
    "to_keep = [i for i in test_vars if i not in noncon_feat]\n",
    "test_x_final = test_x[to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features using sklearn RobustScaler. This scaler defaults to using the interquartile range for the scaling statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "cols = train_x_final.columns\n",
    "scaler = RobustScaler()\n",
    "train_x_final = scaler.fit_transform(train_x_final)\n",
    "test_x_final = scaler.transform(test_x_final)\n",
    "train_x_final = pd.DataFrame(train_x_final, columns=[cols])\n",
    "test_x_final = pd.DataFrame(test_x_final, columns=[cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.Series(train_y)\n",
    "\n",
    "#keep the original pandas dataframe \n",
    "train_x_final_pd = train_x_final\n",
    "test_x_final_pd = test_x_final\n",
    "\n",
    "#convert the pandas dataframe to numpy array \n",
    "train_x_final = train_x_final.to_numpy()\n",
    "test_x_final = test_x_final.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Random Forest classifier from scikit learn to get an idea of what the useful features are and what we can expect for preliminary accuracy. Accuracy was determined from K folds cross validation. Here I also tried simpler models such as logistic regression, svms, K-nearest neighbors all to little/moderate success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8544315459738345\n",
      "0.8557213930348259\n",
      "0.85911729475721\n",
      "mean\n",
      "0.8564234112552902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#try out k folds \n",
    "n_splits = 3 #number of folds\n",
    "acc_arr = np.zeros(n_splits) #preallocate accuracy of each fold \n",
    "\n",
    "kf = sklearn.model_selection.KFold(n_splits = n_splits) #initialize kfolds\n",
    "kf.get_n_splits(train_x_final,train_y) \n",
    "\n",
    "i = 0 \n",
    "for train_index, test_index in kf.split(train_x_final, train_y):\n",
    "    X_train, X_test = train_x_final[train_index], train_x_final[test_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[test_index]\n",
    "            \n",
    "    #test the model \n",
    "    \n",
    "#     #logistic regression\n",
    "#     logreg_model = sklearn.linear_model.LogisticRegression(penalty = 'l2', C = 0.5)\n",
    "#     logreg_model.fit(X_train,y_train)\n",
    "#     model = logreg_model\n",
    "    \n",
    "#     #svm\n",
    "#     model = sklearn.svm.SVC(gamma = 'auto')\n",
    "#     model.fit(X_train,y_train)\n",
    "    \n",
    "#     #  K nearest neighbor\n",
    "#     import sklearn.neighbors\n",
    "#     model = sklearn.neighbors.KNeighborsClassifier(n_neighbors =100, weights='distance', p=1)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     #adaboost #86.4\n",
    "#     from sklearn.ensemble import AdaBoostClassifier\n",
    "#     model = AdaBoostClassifier(n_estimators=100,learning_rate= 1)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "    # random forest #84.2\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators = 100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "#     from sklearn.neural_network import MLPClassifier\n",
    "#     model = MLPClassifier()\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#     from sklearn.gaussian_process.kernels import RBF\n",
    "#     kernel = 1.0 * RBF(1.0)\n",
    "#     gpc = GaussianProcessClassifier(kernel=kernel,\n",
    "#            random_state=0).fit(X_train, y_train)\n",
    "    \n",
    "#     #try out naive bayes\n",
    "#     from sklearn.naive_bayes import GaussianNB\n",
    "#     model = GaussianNB()\n",
    "#     model.fit(X_train, y_train)\n",
    "#     #gradient boosting\n",
    "#     from sklearn.ensemble import GradientBoostingClassifier\n",
    "#     model = GradientBoostingClassifier()\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     from sklearn.linear_model import RidgeCV, LassoCV\n",
    "#     from sklearn.svm import SVR\n",
    "    \n",
    "#     estimators = [('ridge', RidgeCV()),\n",
    "#                   ('lasso', LassoCV(random_state=42)),\n",
    "#                   ('svr', SVR(C=1, gamma=1e-6))]\n",
    "    \n",
    "#     from sklearn.ensemble import GradientBoostingRegressor\n",
    "#     from sklearn.ensemble import StackingRegressor\n",
    "#     model = StackingRegressor(\n",
    "#         estimators=estimators,\n",
    "#         final_estimator=GradientBoostingRegressor(random_state=42))\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "    #determine accuracy of this fold\n",
    "    acc =  sklearn.metrics.accuracy_score(model.predict(X_test), y_test)\n",
    "    acc_arr[i] = acc\n",
    "    print(acc)\n",
    "    i+=1      \n",
    "# print(acc_arr)\n",
    "print('mean')\n",
    "print(np.mean(acc_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the lowest weighted scores according to the random forest classifier feature weights. In addition to speeding up run time, this may also improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fnlwgt                                        1.568733e-01\n",
      "age                                           1.512121e-01\n",
      "capital-gain                                  9.571370e-02\n",
      "hours-per-week                                8.308781e-02\n",
      "marital-status_ Married-civ-spouse            6.173552e-02\n",
      "                                                  ...     \n",
      "native-country_ Honduras                      1.590997e-05\n",
      "workclass_ Without-pay                        1.408628e-05\n",
      "native-country_ Outlying-US(Guam-USVI-etc)    1.280102e-05\n",
      "workclass_ Never-worked                       2.107648e-07\n",
      "native-country_ Holand-Netherlands            0.000000e+00\n",
      "Length: 107, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matsuo/anaconda2/envs/wirms/lib/python3.5/site-packages/pandas/core/generic.py:3946: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  new_axis = axis.drop(labels, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# view the feature scores\n",
    "feature_scores = pd.Series(model.feature_importances_, index=train_x_final_pd.columns).sort_values(ascending=False)\n",
    "print(feature_scores)\n",
    "# feature_scores[-10:]\n",
    "# train_x_final_pd.columns[-10:]\n",
    "\n",
    "#drop least important features\n",
    "train_x_final_pd_2 = train_x_final_pd\n",
    "test_x_final_pd_2 = test_x_final_pd\n",
    "for feature in train_x_final_pd.columns[-5:]:\n",
    "#     pdb.set_trace()\n",
    "    train_x_final_pd_2 = train_x_final_pd_2.drop(feature[0], axis = 1)\n",
    "    test_x_final_pd_2 = test_x_final_pd_2.drop(feature[0], axis = 1)\n",
    "\n",
    "train_x_final = train_x_final_pd_2.to_numpy()\n",
    "test_x_final = test_x_final_pd_2.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retry training the model with much more advanced techniques on the pruned feature set with the same k folds cross validation on the training set. Eventually I settled on using the gradient boosting classifier due to the benefits of ensemble learning and its robustness from overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8703703703703703\n",
      "0.8725815367606412\n",
      "0.8729383580576799\n",
      "mean\n",
      "0.8719634217295639\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#try out k folds \n",
    "n_splits = 3\n",
    "acc_arr = np.zeros(n_splits)\n",
    "kf = sklearn.model_selection.KFold(n_splits = n_splits)\n",
    "kf.get_n_splits(train_x_final,train_y)\n",
    "i = 0 \n",
    "for train_index, test_index in kf.split(train_x_final, train_y):\n",
    "    \n",
    "    X_train, X_test = train_x_final[train_index], train_x_final[test_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[test_index]\n",
    "             \n",
    "#     # random forest #84.2\n",
    "#     from sklearn.ensemble import RandomForestClassifier\n",
    "#     model = RandomForestClassifier(n_estimators = 300)\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "    #gradient boosting classsifier 87.1 \n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "#     model = GradientBoostingClassifier(n_estimators = 300) #this is the kaggle sub\n",
    "    model = GradientBoostingClassifier(n_estimators = 500)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #determine accuracy of this fold\n",
    "    acc =  sklearn.metrics.accuracy_score(model.predict(X_test), y_test)\n",
    "    acc_arr[i] = acc\n",
    "    print(acc)\n",
    "    i+=1      \n",
    "# print(acc_arr)\n",
    "print('mean')\n",
    "print(np.mean(acc_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I train the model on the full dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #train big model \n",
    "model.fit(train_x_final,train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on the test set and output the results in the desired csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the results\n",
    "category = model.predict(test_x_final)\n",
    "category = [int(x) for x in category]\n",
    "output = {'Id':range(len(category)), 'Category': category}\n",
    "\n",
    "output = pd.DataFrame(output)\n",
    "\n",
    "output.to_csv('submission.csv', columns = ['Id', 'Category'],index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16276</th>\n",
       "      <td>0</td>\n",
       "      <td>16276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16277</th>\n",
       "      <td>0</td>\n",
       "      <td>16277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16278</th>\n",
       "      <td>1</td>\n",
       "      <td>16278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16279</th>\n",
       "      <td>0</td>\n",
       "      <td>16279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16280</th>\n",
       "      <td>1</td>\n",
       "      <td>16280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16281 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category     Id\n",
       "0             0      0\n",
       "1             0      1\n",
       "2             0      2\n",
       "3             1      3\n",
       "4             0      4\n",
       "...         ...    ...\n",
       "16276         0  16276\n",
       "16277         0  16277\n",
       "16278         1  16278\n",
       "16279         0  16279\n",
       "16280         1  16280\n",
       "\n",
       "[16281 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the output is sane \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
