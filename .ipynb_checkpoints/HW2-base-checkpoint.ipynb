{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Logistic Regression\n",
    "## CSCI 5622 - Spring 2019\n",
    "***\n",
    "**Name**: $<$insert name here$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11.59 PM on Wednesday, February 26th**. Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your classmates and instructors, but **you must write all code and solutions on your own**, and list any people or sources consulted. The only exception to this rule is that you may copy code directly from your own solution to homework 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "Your task for this homework is to build a logistic regression model that implements stochastic gradient ascent. You'll apply it to the task of determining whether a number is 8 or 9\n",
    "\n",
    "We start by importing and plotting the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ 70 points] Problem 1: Implementing the Logistic Regression Classifier for Binary Classification\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import pickle, gzip       \n",
    "import numpy as np\n",
    "\n",
    "class Numbers:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data for images of 9 and 8 only\n",
    "    \"\"\" \n",
    "    def __init__(self, location):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        with gzip.open(location, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    " \n",
    "        self.train_x, self.train_y = train_set\n",
    "        train_indices = np.where(self.train_y > 7)\n",
    "        self.train_x, self.train_y = self.train_x[train_indices], self.train_y[train_indices]\n",
    "        self.train_y = self.train_y - 8\n",
    " \n",
    "        self.valid_x, self.valid_y = valid_set\n",
    "        valid_indices = np.where(self.valid_y > 7)\n",
    "        self.valid_x, self.valid_y = self.valid_x[valid_indices], self.valid_y[valid_indices]\n",
    "        self.valid_y = self.valid_y - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Numbers('./mnist-1.pklz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you'll implement a Logistic Regression classifier to take drawings of either an eight or a nine and output the corresponding label.\n",
    "* [10 pts] Finish the `calculate_score` function to return the output of applying the dot product of the weights with the input parameter\n",
    "\n",
    "* [10 pts] Finish the `sigmoid` function to return the output of applying the sigmoid function to the calculated score\n",
    "\n",
    "* [10 pts] Finish the `compute_gradient` function to return the derivate of the cost w.r.t. the weights\n",
    "\n",
    "* [10 pts] Finish the `sgd_update` function so that it performs stochastic gradient descent on the single training example and updates the weight vector correspondingly\n",
    "\n",
    "* [10 pts] Finish the `mini_batch_update` function so that it performs mini-batch gradient descent on the batches of the training data set example and updates the weight vector correspondingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class LogReg:\n",
    "    \n",
    "    def __init__(self, X, y, eta = 0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: The number of features (including bias)\n",
    "        :param eta: Learning rate (the default is a constant value)\n",
    "        :method: This should be the name of the method (sgd_update or mini_batch_descent)\n",
    "        :batch_size: optional argument that is needed only in the case of mini_batch_descent\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = np.zeros(X.shape[1]) # can remove from here and ask to be defined in the function\n",
    "        self.eta = eta\n",
    "        \n",
    "    def calculate_score(self, x):\n",
    "        \"\"\"\n",
    "        :param x: This can be a single training example or it could be n training examples\n",
    "        :return score: Calculate the score that you will plug into the logistic function\n",
    "        \"\"\"\n",
    "        # TODO: Compute the score to be fed to the sigmoid function\n",
    "        return np.dot(self.w, x)\n",
    "    \n",
    "    def sigmoid(self, score):\n",
    "        \"\"\"\n",
    "        :param score: Either a real valued number or a vector to convert into a number between 0 and 1\n",
    "        :return sigmoid: Calcuate the output of applying the sigmoid function to the score. This could be a single\n",
    "        value or a vector depending on the input\n",
    "        \"\"\"\n",
    "        # TODO: Complete this function to return the output of applying the sigmoid function to the score\n",
    "        return 1/(1+np.exp(-score))\n",
    "    \n",
    "    def compute_gradient(self, x, h, y):\n",
    "        \"\"\"\n",
    "        :param x: Feature vector\n",
    "        :param h: predicted class label\n",
    "        :param y: real class label\n",
    "        :return gradient: Return the derivate of the cost w.r.t to the weights\n",
    "        \"\"\"\n",
    "        # TODO: Finish this function to compute the gradient\n",
    "        # compute gradient using this: x * (h - y)\n",
    "        gradient=0\n",
    "        if(int(x.size/2)==1):\n",
    "            return (self.sigmoid(self.calculate_score(x))-y)*x\n",
    "        \n",
    "        for i in range(int(x.size/2)):\n",
    "            gradient += (self.sigmoid(self.calculate_score(x[i]))-y)*x[i]\n",
    "        return gradient\n",
    "     \n",
    "    def sgd_update(self):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update over the entire dataset to improve the log likelihood.\n",
    "        :param x_i: The features of the example to take the gradient with respect to\n",
    "        :param y: The target output of the example to take the gradient with respect to\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\" \n",
    "        # TODO: Finish this function to do a stochastic gradient descent update over the entire dataset\n",
    "        # and return the updated weight vector\n",
    "        # w = w - lambda * [1/(1 + e ^ - (w* x_i)) - y_i] * x_ik for k = 0 -> D\n",
    "        for i in range(len(self.X)):\n",
    "#             print(\"sgd \",self.X[i])\n",
    "            self.w = self.w - self.eta * self.compute_gradient(self.X[i],self.y[i],self.y[i])\n",
    "        return self.w\n",
    "    \n",
    "    def mini_batch_update(self, batch_size):\n",
    "        \"\"\"\n",
    "        One iteration of the mini-batch update over the entire dataset (one sweep of the dataset).\n",
    "        :param X: NumPy array of features (size : no of examples X features)\n",
    "        :param y: Numpy array of class labels (size : no of examples X 1)\n",
    "        :param batch_size: size of the batch for gradient update\n",
    "        :returns w: Coefficients of the classifier (after updating)\n",
    "        \"\"\"\n",
    "        # TODO: Performing mini-batch training follows the same steps as in stochastic gradient descent,\n",
    "        # the only major difference is that weâ€™ll use batches of training examples instead of one. \n",
    "        # Here we decide a batch size, which is the number of examples that will be fed into the \n",
    "        # computational graph at once.\n",
    "        for i in range(0,len(self.X),batch_size):\n",
    "            if((i+batch_size)< len(self.X)):\n",
    "                self.w = self.w - self.eta* self.compute_gradient(self.X[i:i+batch_size],self.y[i:i+batch_size],self.y[i:i+batch_size])\n",
    "            else:\n",
    "                self.w = self.w - self.eta* self.compute_gradient(self.X[i:][0],self.y[i:][0],self.y[i:][0])\n",
    "                                           \n",
    "        return self.w\n",
    "    \n",
    "    def progress(self, test_x, test_y, update_method, *batch_size):\n",
    "        \"\"\"\n",
    "        Given a set of examples, computes the probability and accuracy\n",
    "        :param test_x: The features of the test dataset to score\n",
    "        :param test_y: The features of the test \n",
    "        :param update_method: The update method to be used, either 'sgd_update' or 'mini_batch_update'\n",
    "        :param batch_size: Optional arguement to be given only in case of mini_batch_update\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "        # TODO: Complete this function to compute the predicted value for an example based on the logistic value\n",
    "        # and return the log probability and the accuracy of those predictions\n",
    "        score = []\n",
    "        pred = []\n",
    "        if(update_method == 'sgd_update'):\n",
    "            w = self.sgd_update()\n",
    "            log_prob = 0\n",
    "            print(test_y)\n",
    "            for x in test_x:\n",
    "                log_prob += np.log(1/(1+np.exp(-(w[0]*x[0]+w[1]*x[1]))))\n",
    "            if(abs(log_prob)> 0.5):\n",
    "                pred.append(1)\n",
    "            else:\n",
    "                pred.append(0)\n",
    "                    \n",
    "            acc = [1 if(pred[i]==test_y[i]) else 0 for i in range(len(test_y))]\n",
    "        elif(update_method == 'mini_batch_update'):\n",
    "            pass\n",
    "        return (log_prob,acc.count(1)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class LogRegTester(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.X = np.array([[0.1, 0.3 ], [0.4, 0.6], [0.8, 0.1], [0.8, 0.1], [0.5, 0.8]])\n",
    "        self.y = np.array([0,  0, 1, 1,  0])\n",
    "        self.log_reg_classifier_1 = LogReg(self.X, self.y, 0.5)\n",
    "        self.log_reg_classifier_2 = LogReg(self.X, self.y, 0.5)\n",
    "        \n",
    "    def test_sgd_update(self):\n",
    "        \"\"\"\n",
    "        Test sgd_update function from LogReg\n",
    "        \"\"\"\n",
    "        weights = self.log_reg_classifier_1.sgd_update()\n",
    "        self.assertEqual(round(weights[0], 2), 0.16)\n",
    "        self.assertEqual(round(weights[1], 2), -0.37)\n",
    "        \n",
    "    def tests_mini_batch_update(self):\n",
    "        \"\"\"\n",
    "        Test mini_batch_update function from LogReg\n",
    "        \"\"\"\n",
    "        weights = self.log_reg_classifier_2.mini_batch_update(2)\n",
    "        self.assertEqual(round(weights[0], 2), 0.17)\n",
    "        self.assertEqual(round(weights[1], 2), -0.37)\n",
    "        \n",
    "    def tests_progress_sgd_update(self):\n",
    "        \"\"\"\n",
    "        Test progress function from LogReg with method = 'sgd_update'\n",
    "        \"\"\"\n",
    "        self.log_reg_classifier_1 = LogReg(self.X[:4], self.y[:4], 0.5)\n",
    "        log_prob, accuracy = self.log_reg_classifier_1.progress(self.X[4:], self.y[4:], 'sgd_update')\n",
    "        self.assertEqual(round(log_prob, 1), -0.7)  # Changed to round 1.\n",
    "        self.assertEqual(accuracy, 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #BEGIN Workspace\n",
    "    #Add more test functions as required\n",
    "    #HINT - You'll want to make sure your\n",
    "    #END Workspace\n",
    "    \n",
    "tests = LogRegTester()\n",
    "myTests = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(myTests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66271acf9fa2ad5ba37138c6c65d0e4b",
     "grade": true,
     "grade_id": "cell-6dae20a6650e207a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc5fb2bf05afb8b55193714209151048",
     "grade": true,
     "grade_id": "cell-758427c4f39cdc27",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8de3625beec198e3f16aeae05b6343d",
     "grade": true,
     "grade_id": "cell-4faa4c852f8052c5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5a0d847dc62f3be14b1741bd706757f",
     "grade": true,
     "grade_id": "cell-83bae9107e0d2269",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25cfd3959691f614fae03fbe78d14a4b",
     "grade": true,
     "grade_id": "cell-3ef057a9059ef5d4",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** After completing the class above, loop over the training data and perform ___stochastic gradient descent___ for 10 epochs, and five different values of eta range [.0001, .01, .1, .5, 1]. Train your model and do the following:\n",
    "\n",
    "* [2.5 pts] PART A : Create a new classifier object and Using the `progress` method, calculate the accuracy on the __training sets__ at the end of each epoch and store it. Plot the accuracy trends for the different values of eta on same graph.\n",
    "\n",
    "* [2.5 pts] PART B : Create a new classifier object and Using the `progress` method, calculate the accuracy on the __validation set__ at the end of each epoch and store it. Plot the accuracy trends for the different values of eta on same graph.\n",
    "\n",
    "Don't forget to shuffle your training data after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement part A below. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dab6194829c3f760a0700843b8ac0d14",
     "grade": true,
     "grade_id": "cell-ef8affc8f47de848",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement part B below. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb60d22338d71c2da37a063263b109c",
     "grade": true,
     "grade_id": "cell-ec6999395608d8f1",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** After completing the class above, loop over the training data and perform ___mini batch gradient descent___ for 10 epochs, and five different values of eta range [.0001, .01, .1, .5, 1]. Train your model and do the following:\n",
    "\n",
    "* [2.5 pts] PART C : Create a new classifier object and Using the `progress` method, calculate the accuracy on the __training sets__ at the end of each epoch and store it. Plot the accuracy trends for the different values of eta on same graph.\n",
    "\n",
    "* [2.5 pts] PART D : Create a new classifier object and Using the `progress` method, calculate the accuracy on the __validation set__ at the end of each epoch and store it. Plot the accuracy trends for the different values of eta on same graph.\n",
    "\n",
    "Don't forget to shuffle your training data after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement part C below. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aed5aa63ce920693892ad8525ba0174",
     "grade": true,
     "grade_id": "cell-52542793c69f29c3",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement part D below. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6476b7a70a099d872b9939d68f8da698",
     "grade": true,
     "grade_id": "cell-d02f5de2e6a3f212",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** [5 pts] Describe the role of learning rate (eta) on the efficiency of convergence during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4803267a54ce015791ce3a8f6ac01e7a",
     "grade": true,
     "grade_id": "cell-672c3d3d77a18ef1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** [5 pts] Describe the role of the number of epochs on validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "673287df57a6b4ddc1a7073723c5686a",
     "grade": true,
     "grade_id": "cell-2aebead81356b6ec",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ 30 points] Problem 2: Implementing the Logistic Regression Classifier for Multinomial Classification\n",
    "\n",
    "You will not create a classifier that is commonly referred to as Multinomial Logistic Regression. The particular method you will be implementing is **One Vs All** or **One Vs Rest**. The dataset will be the MNIST dataset which includes all digits 0-9. You are free to use the functions you created above as needed.\n",
    "\n",
    "* [5 pts] 2.1 Normalize your data.\n",
    "* [5 pts] 2.2 Transform your outputs into a set of binary features via one-hot encoding.\n",
    "* [5 pts] 2.3 Write get_optimal_parameters - train all ten models at once.\n",
    "* [5 pts] 2.4 Calculate the accuracy of your model on Train and Test data.\n",
    "* [5 pts] 2.5 Generate a confusion matrix on test and train Data.\n",
    "* [5 pts] 2.6 Predict the labels of the first ten datapoints from your test set alongside the images of those same datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Numbers2:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data for images of 0-9\n",
    "    \"\"\" \n",
    "    def __init__(self, location):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        with gzip.open(location, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    " \n",
    "        self.train_x, self.train_y = train_set\n",
    "        self.test_x, self.test_y = valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = Numbers2('./mnist.pklz')\n",
    "print(data2.train_y[:10])\n",
    "def view_digit(example, label=None):\n",
    "    if label is not None: print(\"true label: {:d}\".format(label))\n",
    "    plt.imshow(example.reshape(28,28), cmap='gray');\n",
    "view_digit(data2.train_x[18],data2.train_y[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "class MultiLogReg:\n",
    "    \n",
    "    def __init__(self, X, y, eta = 0.1):\n",
    "        self.X = self.normalize_data(X)\n",
    "        self.y = self.one_hot_encoding(y)\n",
    "        self.eta = eta\n",
    "        \n",
    "    def one_hot_encoding(self, y):\n",
    "        # TO DO: Represent the output vector y as a one hot encoding. Create a matrix of dimensions (m X 10) \n",
    "        # where m = number of examples, and 10 for number of classes\n",
    "        # if the class for the ith example is 7, then y[i][7] = 1 and the for k != 7, y[i][k] = 0.\n",
    "    \n",
    "        \n",
    "    def normalize_data(self, X):\n",
    "        # TO DO: Normalize the feature values of dataset X using the mean and standard deviation of the respective features \n",
    "\n",
    "        \n",
    "    def get_optimal_parameters(self):\n",
    "        # TO DO: This is the main training loop. You will have to find the optimal weights for all 10 models\n",
    "        # Each model is fit to it's class which is (0-9), and the cost function will be against all of the other \n",
    "        # numbers \"the rest\".\n",
    "\n",
    "    \n",
    "    def predict(self, test_image, test_label):\n",
    "        # TO DO: This function should return the probabilities predicted by each of the models for some given \n",
    "        # input image. The probabilities are sorted with the most likely being listed first.\n",
    "        # Return a vector of shape (10, 2) with the first column holding the number and the second column with\n",
    "        # the probability that the test_image is that number\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a9848ee01129a955b176d30e8b5ad7b",
     "grade": true,
     "grade_id": "cell-6b2210e74f243cc5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0864f8ccc23364600d23825d74b4b805",
     "grade": true,
     "grade_id": "cell-c999e8642ad56c95",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION ###\n",
    "It is important to know how well your model did on the whole. You need to report the ___accuracy as a percentage___ on the training set and the test set from Numbers2. You should also plot a ___confusion matrix___ for both, just like you did on the last homework and mention the numbers that were misclassified the most. Finally use the predict method to predict the labels for the FIRST 10 instances from the test set, while also plotting the respective images for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a MultiLogReg Model in the cell below. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6adc8b9ff983ac7fb1d90033bb0841fa",
     "grade": true,
     "grade_id": "cell-5a678a51a11b6ce8",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy on the train and test data from Numbers2 using the model you trained above. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7216ab7a873dd1839e1ba8b783aec16e",
     "grade": true,
     "grade_id": "cell-f82d99c679208a96",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix on Test and Train Data using the model you trained before. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12ad7b3162aa14d26a16359ea6b937e7",
     "grade": true,
     "grade_id": "cell-99273a4c41cde194",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Labels of First 10 datapoints from your test set and also the plot their images in the cell below. DO NOT DELETE THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "357ff8dee98adb78a768e9f690b3245d",
     "grade": true,
     "grade_id": "cell-2ae148d5cb9b50b2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
