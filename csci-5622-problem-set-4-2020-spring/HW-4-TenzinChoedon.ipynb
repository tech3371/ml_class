{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import collections\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['age','workclass','fnlwgt','education','education-num','marital-status',\n",
    "               'occupation','relationship','race','sex','capital-gain','capital-loss',\n",
    "               'hours-per-week','native-country']\n",
    "df = pd.read_csv('train-features.csv',names=columns_name,header=None,na_values=' ?')\n",
    "df['income'] = pd.read_csv('train-output.csv') #Binary (0 means <=50K, 1 means >50K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = None\n",
    "        self.columns_name = ['age','workclass','fnlwgt','education','education-num',\n",
    "                                       'marital-status','occupation','relationship','race','sex',\n",
    "                                       'capital-gain','capital-loss','hours-per-week','native-country']\n",
    "        self.train_data = pd.read_csv('train-features.csv',names=self.columns_name,header=None,na_values=' ?')\n",
    "        self.train_label = pd.read_csv('train-output.csv',names=['income'],header=None) #Binary (0 means <=50K, 1 means >50K)\n",
    "        self.train_label = self.train_label.values.ravel()\n",
    "        self.test_data = pd.read_csv('test-features.csv',names=self.columns_name,header=None,na_values=' ?')\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test  = None\n",
    "        self.y_train = None\n",
    "        self.y_test  = None\n",
    "    def preprocess(self,df):\n",
    "        '''\n",
    "        Cleans df and performs feature engineering.\n",
    "        '''\n",
    "        # replace nan values with random values of it's column's value\n",
    "        for i in self.columns_name:\n",
    "            df[i] = df[i].fillna(np.random.choice(df[i].dropna().unique()))\n",
    "\n",
    "        # category\n",
    "        df['education'] = df['education'].str.replace('Preschool', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('10th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('11th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('12th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('1st-4th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('5th-6th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('7th-8th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('9th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('HS-Grad', 'HighGrad')\n",
    "        df['education'] = df['education'].str.replace('HS-grad', 'HighGrad')\n",
    "        df['education'] = df['education'].str.replace('Some-college', 'CommunityCollege')\n",
    "        df['education'] = df['education'].str.replace('Assoc-acdm', 'CommunityCollege')\n",
    "        df['education'] = df['education'].str.replace('Assoc-voc', 'CommunityCollege')\n",
    "        \n",
    "        # [' Never-married', ' Married-civ-spouse', ' Divorced',' Married-spouse-absent', ' Separated', ' Married-AF-spouse',\n",
    "        # ' Widowed']\n",
    "        df['marital-status'] = df['marital-status'].str.replace('Never-married','notMarried')\n",
    "        df['marital-status'] = df['marital-status'].str.replace('Seperated','notMarried')\n",
    "        df['marital-status'] = df['marital-status'].str.replace('Divorced','Seperated')\n",
    "        df['marital-status'] = df['marital-status'].str.replace('Seperated','Seperated')\n",
    "        df['marital-status'] = df['marital-status'].str.replace('Married-AF-spouse','Married')\n",
    "        df['marital-status'] = df['marital-status'].str.replace('Married-civ-spouse','Married')\n",
    "        df['marital-status'] = df['marital-status'].str.replace('Married-spouse-absent','Married')\n",
    "        \n",
    "        # Binning\n",
    "        \n",
    "        df.loc[df['education-num']  < 9,'education-num']   = 0 # dropout\n",
    "        df.loc[df['education-num'] == 9,'education-num']  = 1 # high school\n",
    "        df.loc[df['education-num'] == 10,'education-num'] = 2 # Community College\n",
    "        df.loc[df['education-num'] == 11,'education-num'] = 2 # Community College\n",
    "        df.loc[df['education-num'] == 12,'education-num'] = 2 # Community College\n",
    "        df.loc[df['education-num'] == 13,'education-num'] = 3 # Bachelor\n",
    "        df.loc[df['education-num'] == 14,'education-num'] = 4 # Master\n",
    "        df.loc[df['education-num'] == 15,'education-num'] = 5 # Prof-school\n",
    "        df.loc[df['education-num'] == 16,'education-num'] = 6 # Doctorate\n",
    "        \n",
    "        # binary\n",
    "        df.loc[df['capital-gain'] >= 2000,'capital-gain'] = 1\n",
    "        df.loc[df['capital-gain']  < 2000,'capital-gain'] = 0\n",
    "        \n",
    "        df.loc[df['capital-loss']  <= 600,'capital-loss'] = 1\n",
    "        df.loc[df['capital-loss']  > 600, 'capital-loss'] = 0\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    def take_model(self,model):\n",
    "        '''\n",
    "        Takes in model.\n",
    "        '''\n",
    "        self.model = model\n",
    "    def split_data(self,df):\n",
    "        \n",
    "        self.x_train,self.x_test, self.y_train, self.y_test = model_selection.train_test_split(df_data,df_label,test_size=0.2,random_state=2)\n",
    "\n",
    "        \n",
    "    def plain_training(self,df_data,df_label,test_df):\n",
    "        '''\n",
    "        It trains model once and returns accuracy score\n",
    "        '''\n",
    "        self.x_train,self.x_test, self.y_train, self.y_test = model_selection.train_test_split(df_data,df_label,test_size=0.2,random_state=2)\n",
    "        self.x_train = self.normalize_data(self.x_train)\n",
    "        self.x_test = self.normalize_data(self.x_test)\n",
    "        test = self.normalize_data(test_df)\n",
    "        clf = self.model()\n",
    "        clf.fit(self.x_train,self.y_train)\n",
    "        y_pred = clf.predict(test)\n",
    "        \n",
    "        return clf.score(self.x_test,self.y_test)\n",
    "    def generate_submission(self, y_pred):\n",
    "        '''\n",
    "        Saves submission in the right format.\n",
    "        '''\n",
    "        pred_df = pd.DataFrame()\n",
    "        pred_df['Id'] = np.arange(0,len(y_pred))\n",
    "        pred_df['Category'] = y_pred\n",
    "        pred_df.to_csv('submission.csv',index=False)\n",
    "        \n",
    "    def encode_df(self,df):\n",
    "        '''\n",
    "        Encode string values in dataframe into numbers.\n",
    "        '''\n",
    "        \n",
    "        return df.apply(LabelEncoder().fit_transform)\n",
    "    def normalize_data(self, X):\n",
    "        # TO DO: Normalize the feature values of dataset X using the mean and standard deviation of the respective features \n",
    "    \n",
    "        return preprocessing.scale(X)\n",
    "    def one_hotencoder(self,df):\n",
    "        ''' Input data should be in number (encoded)'''\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        onehotencoder = OneHotEncoder()\n",
    "        data = onehotencoder.fit_transform(df).toarray()\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def kbins(self,df,bins):\n",
    "        from sklearn.preprocessing import KBinsDiscretizer\n",
    "        est = KBinsDiscretizer(n_bins=bins,encode='onehot-dense',strategy='kmeans')\n",
    "        est.fit(df)\n",
    "        Xt = est.transform(df)\n",
    "        return Xt\n",
    "    def kfold_cw(self,model,df_data,df_label,niter):\n",
    "        from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "        score = cross_val_score(model,df_data,df_label,cv=niter)\n",
    "        pred  = cross_val_predict(model,df_data,df_label,cv=niter)\n",
    "        \n",
    "        return np.mean(score),pred\n",
    "    def randomforest_cw(self,train_df,test_df):\n",
    "        '''\n",
    "        Cross-validation training for Random Forest method.\n",
    "        '''\n",
    "        from sklearn.model_selection import KFold\n",
    "        \n",
    "        kfold = KFold(n_splits=5)\n",
    "        n_estimators = [int(x) for x in np.linspace(200,2000,10)]\n",
    "        for i in n_estimators:\n",
    "            # resample train and test data. Normalize test data in this step as well.\n",
    "            self.x_train,self.x_test, self.y_train, self.y_test = model_selection.train_test_split(train_df[self.columns_name],train_df['income'],test_size=0.2,random_state=2)\n",
    "            self.x_train = self.normalize_data(self.x_train)\n",
    "            self.x_test = self.normalize_data(self.x_test)\n",
    "            \n",
    "            # k-fold split again\n",
    "            \n",
    "            # train and fit on random forest method\n",
    "            randomforest = ensemble.RandomForestClassifier(max_depth=20, random_state=0)\n",
    "            randomforest.fit(self.x_train,self.y_train)\n",
    "            \n",
    "            # store n_estimator and it's accuracy value\n",
    "            \n",
    "        \n",
    "        test = self.normalize_data(test_df[selected])\n",
    "        \n",
    "        y_pred = randomforest.predict(test)\n",
    "        return y_pred, randomforest.score(self.x_test,self.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "# clf.take_model(naive_bayes.GaussianNB)\n",
    "# clf.plain_training(encoded_df,encoded_test_df,test_columns)\n",
    "# x_train,x_test, y_train, y_test = model_selection.train_test_split(df_norm,clf.train_label,test_size=0.2,random_state=2)\n",
    "\n",
    "# NB = naive_bayes.GaussianNB()\n",
    "# NB.fit(x_train,y_train)\n",
    "# y_pred = NB.predict(x_test)\n",
    "# accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8134500230308613"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Classification()\n",
    "df_data = clf.train_data\n",
    "df_label = clf.train_label\n",
    "df = clf.preprocess(df_data)\n",
    "\n",
    "test_df = clf.test_data\n",
    "test_df = clf.preprocess(test_df)\n",
    "\n",
    "encoded_df = clf.encode_df(df_data)\n",
    "encoded_test_df = clf.encode_df(test_df)\n",
    "\n",
    "# kbins = clf.kbins(encoded_df,5)\n",
    "test_columns = ['age','workclass','fnlwgt','education-num','marital-status',\n",
    "               'occupation','relationship','race','sex','capital-gain','capital-loss',\n",
    "               'hours-per-week']\n",
    "encoded_df.head()\n",
    "# print(encoded_df.head())\n",
    "# print(clf.train_label.head())\n",
    "df_norm = clf.normalize_data(encoded_df)\n",
    "df_test_norm = clf.normalize_data(encoded_test_df)\n",
    "df[:10]\n",
    "clf.take_model(ensemble.RandomForestClassifier)\n",
    "# clf.model\n",
    "clf.plain_training(df_norm,clf.train_label,df_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "# {'n_estimators': 1600,\n",
    "#  'min_samples_split': 2,\n",
    "#  'min_samples_leaf': 4,\n",
    "#  'max_features': 'sqrt',\n",
    "#  'max_depth': 10,\n",
    "#  'bootstrap': True}\n",
    "rdmforest = ensemble.RandomForestClassifier(n_estimators=1600,min_samples_split=2,min_samples_leaf=4,max_depth=10,bootstrap=True)\n",
    "\n",
    "score = cross_val_score(rdmforest,df_norm,df_label,cv=3)\n",
    "print('prediction')\n",
    "pred  = cross_val_predict(rdmforest,df_norm,df_label,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83618942 0.83812419 0.84059707]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([26802,  5759]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(score)\n",
    "np.unique(pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/411290/how-to-use-a-cross-validated-model-for-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = rdmforest.fit(df_norm,df_label)\n",
    "new_pred = rdmforest.predict(df_test_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred.shape\n",
    "clf.generate_submission(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([13466,  2815]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(new_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.30673797398967845,\n",
       " array([0.03965261, 0.22632972, 0.03198692, ..., 0.04392514, 0.07944159,\n",
       "        0.26698445]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "best_svr = SVR(kernel='rbf',gamma='auto')\n",
    "clf.kfold_cw(best_svr,df_norm,clf.train_label,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.7min finished\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1600,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 10,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest parameter\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# now we instantiate the random search and fit it like any Sklearn model\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(encoded_df[:100][columns_name], encoded_df[:100]['income'])\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6033, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6033,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.split_data(encoded_df)\n",
    "print(clf.x_test.shape)\n",
    "clf.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.2458 degrees.\n",
      "Accuracy = -inf%.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy,predictions\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "test_features = clf.x_test\n",
    "test_features = clf.encode_df(test_features)\n",
    "test_labels = clf.y_test\n",
    "# test_labels = clf.encode_df(test_labels)\n",
    "random_accuracy,pred = evaluate(best_random, test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_random.predict(encoded_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16281,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from sklearn.svm import SVC\n",
    "clf.take_model(SVC)\n",
    "clf.plain_training(encoded_df,encoded_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00010095938492371559"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.take_model(linear_model.Lasso)\n",
    "clf.plain_training(encoded_df,encoded_test_df,test_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 3,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of \n",
    "randomforest = ensemble.RandomForestClassifier(max_depth=3, random_state=0)\n",
    "randomforest.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5701972484667661"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model = Perceptron(tol=1e-3,random_state=0,penalty='l2',max_iter=20)\n",
    "model.fit(clf.x_train,clf.y_train)\n",
    "model.score(clf.x_test,clf.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5701972484667661"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(clf.x_test,clf.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
