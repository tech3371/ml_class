{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import collections\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['age','workclass','fnlwgt','education','education_num','marital-status',\n",
    "               'occupation','relationship','race','sex','capital-gain','capital-loss',\n",
    "               'hours-per-week','native-country']\n",
    "df = pd.read_csv('train-features.csv',names=columns_name,header=None,na_values=' ?')\n",
    "df['income'] = pd.read_csv('train-output.csv') #Binary (0 means <=50K, 1 means >50K)\n",
    "df.columns = df.columns.str.replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov',\n",
       "       ' Local-gov', nan, ' Self-emp-inc', ' Without-pay',\n",
       "       ' Never-worked'], dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.workclass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = None\n",
    "        self.columns_name = ['age','workclass','fnlwgt','education','education_num',\n",
    "                                       'marital_status','occupation','relationship','race','sex',\n",
    "                                       'capital_gain','capital_loss','hours_per_week','native_country']\n",
    "        self.train_data = pd.read_csv('train-features.csv',names=self.columns_name,header=None,na_values=' ?')\n",
    "        self.train_label = pd.read_csv('train-output.csv',names=['income'],header=None) #Binary (0 means <=50K, 1 means >50K)\n",
    "#         self.train_label = self.train_label.values.ravel()\n",
    "        self.test_data = pd.read_csv('test-features.csv',names=self.columns_name,header=None,na_values=' ?')\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test  = None\n",
    "        self.y_train = None\n",
    "        self.y_test  = None\n",
    "    def preprocess(self,df,bins):\n",
    "        '''\n",
    "        Cleans df and performs feature engineering.\n",
    "        '''\n",
    "        # replace nan values with most commone values of it's column\n",
    "        for i in self.columns_name:\n",
    "            df[i] = df[i].fillna(list(dict(df_data['education'].dropna().value_counts()))[0])\n",
    "\n",
    "        # category\n",
    "        df['education'] = df['education'].str.replace('Preschool', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('10th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('11th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('12th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('1st-4th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('5th-6th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('7th-8th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('9th', 'dropout')\n",
    "        df['education'] = df['education'].str.replace('HS-Grad', 'HighGrad')\n",
    "        df['education'] = df['education'].str.replace('HS-grad', 'HighGrad')\n",
    "        df['education'] = df['education'].str.replace('Some-college', 'CommunityCollege')\n",
    "        df['education'] = df['education'].str.replace('Assoc-acdm', 'CommunityCollege')\n",
    "        df['education'] = df['education'].str.replace('Assoc-voc', 'CommunityCollege')\n",
    "        \n",
    "        # [' Never-married', ' Married-civ-spouse', ' Divorced',' Married-spouse-absent', ' Separated', ' Married-AF-spouse',\n",
    "        # ' Widowed']\n",
    "        df['marital_status'] = df['marital_status'].str.replace('Never-married','notMarried')\n",
    "        df['marital_status'] = df['marital_status'].str.replace('Seperated','notMarried')\n",
    "        df['marital_status'] = df['marital_status'].str.replace('Divorced','notMarried')\n",
    "        df['marital_status'] = df['marital_status'].str.replace('Seperated','notMarried')\n",
    "        df['marital_status'] = df['marital_status'].str.replace('Married-AF-spouse','Married')\n",
    "        df['marital_status'] = df['marital_status'].str.replace('Married-civ-spouse','Married')\n",
    "        df['marital_status'] = df['marital_status'].str.replace('Married-spouse-absent','notMarried')\n",
    "        \n",
    "        # df['workclass'] doesn't effect performance\n",
    "        \n",
    "        # Binning\n",
    "        education_num = pd.cut(df.education_num,bins=[0,8,9,12,13,14,15,16],labels=[0,1,2,3,4,5,6])\n",
    "        df['education_num'] = education_num\n",
    "        \n",
    "        # Create categories for continuous values. Run it only once\n",
    "#         age = pd.cut(df.age,bins=[0,18,27,50,100],labels=[0,1,2,3]) # ['HighGrad','Bachelor','Grad','others'] ==> https://en.wikipedia.org/wiki/Education_in_the_United_States\n",
    "        age = pd.cut(df.age,bins)  \n",
    "        hours = pd.cut(df.hours_per_week,bins=[0,20,40,100],labels=['PT','FT','OverTime'])\n",
    "        \n",
    "        # replace old column with new column with category values\n",
    "        df['age'] = age\n",
    "        df['hours_per_week'] = hours\n",
    "        \n",
    "        df\n",
    "        return df\n",
    "    def take_model(self,model):\n",
    "        '''\n",
    "        Takes in model.\n",
    "        '''\n",
    "        self.model = model\n",
    "        \n",
    "    def split_data(self,df_data,df_label):\n",
    "        \n",
    "        self.x_train,self.x_test, self.y_train, self.y_test = model_selection.train_test_split(df_data,df_label,test_size=0.2,random_state=2)\n",
    "\n",
    "        \n",
    "    def plain_training(self,df_data,df_label,test_df):\n",
    "        '''\n",
    "        It trains model once and returns accuracy score\n",
    "        '''\n",
    "        self.x_train,self.x_test, self.y_train, self.y_test = model_selection.train_test_split(df_data,df_label,test_size=0.2,random_state=2)\n",
    "        self.x_train = self.normalize_data(self.x_train)\n",
    "        self.x_test = self.normalize_data(self.x_test)\n",
    "        test = self.normalize_data(test_df)\n",
    "        clf = self.model()\n",
    "        clf.fit(self.x_train,self.y_train)\n",
    "        y_pred = clf.predict(test)\n",
    "        \n",
    "        return clf.score(self.x_test,self.y_test)\n",
    "    \n",
    "    def generate_submission(self, y_pred):\n",
    "        '''\n",
    "        Saves submission in the right format.\n",
    "        '''\n",
    "        pred_df = pd.DataFrame()\n",
    "        pred_df['Id'] = np.arange(0,len(y_pred))\n",
    "        pred_df['Category'] = y_pred\n",
    "        pred_df.to_csv('submission.csv',index=False)\n",
    "        \n",
    "    def encode_df(self,df):\n",
    "        '''\n",
    "        Encode string values in dataframe into numbers.\n",
    "        '''\n",
    "        \n",
    "        return df.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    def normalize_data(self, X):\n",
    "        # TO DO: Normalize the feature values of dataset X using the mean and standard deviation of the respective features \n",
    "        # min_max has is better than robustscaler\n",
    "#         min_max_scaler = preprocessing.MinMaxScaler() \n",
    "#         X = min_max_scaler.fit_transform(X)\n",
    "        \n",
    "#         scaler = preprocessing.RobustScaler()\n",
    "#         X = scaler.fit_transform(X)\n",
    "        # scale is better than min_max\n",
    "        X = preprocessing.scale(X)\n",
    "#         X = preprocessing.normalize(X)\n",
    "        return X\n",
    "    \n",
    "    def kbins(self,df,bins):\n",
    "        from sklearn.preprocessing import KBinsDiscretizer\n",
    "        est = KBinsDiscretizer(n_bins=bins,encode='onehot-dense',strategy='kmeans')\n",
    "        est.fit(df)\n",
    "        Xt = est.transform(df)\n",
    "        \n",
    "        return Xt\n",
    "    def panda_onehotencode(self,train,test,selected):\n",
    "        '''\n",
    "        One hot encode train and test data. Then make sure they have same column amount.\n",
    "        # https://stackoverflow.com/questions/58101126/using-scikit-learn-onehotencoder-with-a-pandas-dataframe\n",
    "        https://towardsdatascience.com/encoding-categorical-features-21a2651a065c\n",
    "        \n",
    "        returns train, test\n",
    "        '''\n",
    "        tmp = pd.get_dummies(train[selected],prefix_sep='_',columns=selected)\n",
    "        print(\"train data shape: \",tmp.shape)\n",
    "\n",
    "        tmp1 = pd.get_dummies(test[selected],prefix_sep='_',columns=selected)\n",
    "        print(\"test data shape: \",tmp1.shape)\n",
    "\n",
    "        final_train, final_test = tmp.align(tmp1, join='inner', axis=1)  # inner join\n",
    "        print(\"final train data : {}\\nfinal test data: {}\".format(final_train.shape,final_test.shape))\n",
    "        return final_train,final_test\n",
    "    \n",
    "    def category_labelencoder(self,data):\n",
    "        '''\n",
    "        convert categories into something.\n",
    "        https://towardsdatascience.com/encoding-categorical-features-21a2651a065c\n",
    "        \n",
    "        But I don't know how to make test and train with same column shape.\n",
    "        '''\n",
    "        tmp = data\n",
    "        # print(df_data.head() )\n",
    "        categorical_feature_mask = tmp.dtypes==object # gets features with string or object value\n",
    "        categorical_feature_mask\n",
    "        # filter categorical columns using mask and turn it into a list\n",
    "        categorical_cols = tmp.columns[categorical_feature_mask].tolist()\n",
    "        categorical_cols\n",
    "\n",
    "        # instantiate labelencoder object\n",
    "        le = LabelEncoder()\n",
    "        # apply le on categorical feature columns\n",
    "        tmp[categorical_cols] = tmp[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "        tmp[categorical_cols].head(10)\n",
    "\n",
    "        # instantiate OneHotEncoder\n",
    "        ohe = OneHotEncoder(categorical_features = categorical_feature_mask, sparse=False ) \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "\n",
    "        # apply OneHotEncoder on categorical feature columns\n",
    "        test_ohe = ohe.fit_transform(tmp) # It returns an numpy array\n",
    "        return test_ohe\n",
    "    def kfold_cw(self,model,df_data,df_label,df_test,niter):\n",
    "        scores = cross_val_score(model,df_data,df_label.values.ravel(),cv=niter)\n",
    "        print(\"Accuracy score: \",scores)\n",
    "#         pred  = cross_val_predict(model,df_data,df_label,cv=niter)\n",
    "        print('fitting now')\n",
    "        final_model = model.fit(df_data,df_label.values.ravel())\n",
    "        print('final prediction')\n",
    "        new_pred = model.predict(df_test)\n",
    "        print('acc: ',np.mean(scores))\n",
    "        return np.mean(scores),new_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classification()\n",
    "\n",
    "# convert train label to dataframe\n",
    "label = clf.train_label.values.ravel()\n",
    "df_label = pd.DataFrame(data=label,columns=['income'])\n",
    "df_label.shape\n",
    "\n",
    "# train data\n",
    "df_data = clf.train_data\n",
    "df_data = clf.preprocess(df_data,20)\n",
    "\n",
    "\n",
    "# test data\n",
    "test_df = clf.test_data\n",
    "test_df = clf.preprocess(test_df,20)\n",
    "\n",
    "\n",
    "\n",
    "# encode both datas\n",
    "encoded_df = clf.encode_df(df_data)\n",
    "encoded_test_df = clf.encode_df(test_df)\n",
    "\n",
    "\n",
    "# normalize train and test data\n",
    "df_norm = clf.normalize_data(encoded_df)\n",
    "df_test_norm = clf.normalize_data(encoded_test_df)\n",
    "# df_norm = clf.kbins(encoded_df,5)\n",
    "# df_test_norm = clf.kbins(encoded_test_df,5)\n",
    "\n",
    "# convert from numpy to dataframe\n",
    "df_norm = pd.DataFrame(data=df_norm,columns=df_data.columns)\n",
    "df_test_norm = pd.DataFrame(data=df_test_norm,columns=df_data.columns)\n",
    "\n",
    "# # finds correlation between features\n",
    "# temp_df = pd.concat([df_norm,clf.train_label],axis=1,sort=False)\n",
    "\n",
    "# plt.figure(figsize=(12,10))\n",
    "# corr = temp_df.corr()\n",
    "# sns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\n",
    "\n",
    "# #Correlation with output variable\n",
    "# cor_target = abs(corr[\"income\"])\n",
    "\n",
    "# #Selecting highly correlated features\n",
    "# relevant_features = cor_target[cor_target>=0.04]\n",
    "# selected = list(dict(relevant_features))\n",
    "# selected = selected[:-1]\n",
    "# print(selected)\n",
    "selected = ['age', 'workclass', 'education_num',\n",
    "       'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "       'capital_gain', 'capital_loss', 'hours_per_week', 'native_country']\n",
    "\n",
    "final_train = df_norm[selected]\n",
    "final_test = df_test_norm[selected]\n",
    "\n",
    "# for i in selected:\n",
    "#     print(i,df_data[i].unique())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
       "       'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
       "       'capital_gain', 'capital_loss', 'hours_per_week', 'native_country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_test_df[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = pd.get_dummies(encoded_df[selected],prefix_sep='_',columns=selected)\n",
    "# for i in selected:\n",
    "#     tmp = pd.get_dummies(encoded_df,prefix=[i], prefix_sep='_',columns = [i], drop_first=True)\n",
    "# print(\"train data shape: \",tmp.shape)\n",
    "# for i in selected:\n",
    "#     tmp1[i] = pd.get_dummies(encoded_test_df,prefix=[i], prefix_sep='_', columns = [i], drop_first=True)\n",
    "# print(\"test data shape: \",tmp.shape)\n",
    "# final_train, final_test = tmp.align(tmp1, join='inner', axis=1)  # inner join\n",
    "# print(\"final train data : {}\\nfinal test data: {}\".format(final_train.shape,final_test.shape))\n",
    "# print(encoded_df.capital_gain.unique())\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  (32561, 314)\n",
      "test data shape:  (16281, 297)\n",
      "final train data : (32561, 68)\n",
      "final test data: (16281, 68)\n"
     ]
    }
   ],
   "source": [
    "# final_train,final_test = clf.panda_onehotencode(final_train,final_test,selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32557</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32558</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32560</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  workclass  education_num  marital_status  occupation  \\\n",
       "0      0.333333      0.875       0.500000        1.000000    0.000000   \n",
       "1      0.444444      0.750       0.500000        0.000000    0.214286   \n",
       "2      0.222222      0.500       0.166667        1.000000    0.428571   \n",
       "3      0.444444      0.500       0.000000        0.000000    0.428571   \n",
       "4      0.111111      0.500       0.500000        0.000000    0.714286   \n",
       "...         ...        ...            ...             ...         ...   \n",
       "32556  0.111111      0.500       0.333333        0.000000    0.928571   \n",
       "32557  0.333333      0.500       0.166667        0.000000    0.500000   \n",
       "32558  0.555556      0.500       0.166667        0.666667    0.000000   \n",
       "32559  0.000000      0.500       0.166667        1.000000    0.000000   \n",
       "32560  0.444444      0.625       0.166667        0.000000    0.214286   \n",
       "\n",
       "       relationship  race  sex  capital_gain  capital_loss  hours_per_week  \\\n",
       "0               0.2   1.0  1.0      0.211864           0.0             0.0   \n",
       "1               0.0   1.0  1.0      0.000000           0.0             1.0   \n",
       "2               0.2   1.0  1.0      0.000000           0.0             0.0   \n",
       "3               0.0   0.5  1.0      0.000000           0.0             0.0   \n",
       "4               1.0   0.5  0.0      0.000000           0.0             0.0   \n",
       "...             ...   ...  ...           ...           ...             ...   \n",
       "32556           1.0   1.0  0.0      0.000000           0.0             0.0   \n",
       "32557           0.0   1.0  1.0      0.000000           0.0             0.0   \n",
       "32558           0.8   1.0  0.0      0.000000           0.0             0.0   \n",
       "32559           0.6   1.0  1.0      0.000000           0.0             1.0   \n",
       "32560           1.0   1.0  0.0      0.915254           0.0             0.0   \n",
       "\n",
       "       native_country  \n",
       "0            0.951220  \n",
       "1            0.951220  \n",
       "2            0.951220  \n",
       "3            0.951220  \n",
       "4            0.097561  \n",
       "...               ...  \n",
       "32556        0.951220  \n",
       "32557        0.951220  \n",
       "32558        0.951220  \n",
       "32559        0.951220  \n",
       "32560        0.951220  \n",
       "\n",
       "[32561 rows x 12 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.836480884385076"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # normalize train and test data\n",
    "# df_norm = clf.normalize_data(encoded_df[selected])\n",
    "# df_test_norm = clf.normalize_data(encoded_test_df[selected])\n",
    "\n",
    "# # convert from numpy to dataframe\n",
    "# df_norm = pd.DataFrame(data=df_norm,columns=selected)\n",
    "# df_test_norm = pd.DataFrame(data=df_test_norm,columns=selected)\n",
    "\n",
    "# print(df_norm)\n",
    "clf.take_model(ensemble.RandomForestClassifier)\n",
    "clf.plain_training(final_train,label,final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique count of prediction\n",
    "# clf.generate_submission(gradient.predict(df_test_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running CV with different method and in the order accuracy score\n",
    "\n",
    "Below k-fold CV uses \n",
    "\n",
    "`cross_val_score`\n",
    "\n",
    "`cross_val_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  [0.8684356  0.87368712 0.87708468]\n",
      "fitting now\n",
      "final prediction\n",
      "acc:  0.8730691322608269\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "gradient = ensemble.GradientBoostingClassifier(n_estimators=200,learning_rate=0.2)\n",
    "gdb_score,gdb_pred = clf.kfold_cw(gradient,final_train,df_label,final_test,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare(pred):\n",
    "    df = pd.read_csv('submission1.csv')\n",
    "    mis_classified = 0\n",
    "    for i in range(len(pred)):\n",
    "        if(df['Category'][i] != pred[i]):\n",
    "            mis_classified += 1\n",
    "    return mis_classified\n",
    "compare(gdb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  [0.86170997 0.8661323  0.86989772]\n",
      "fitting now\n",
      "final prediction\n",
      "acc:  0.8659133314208013\n"
     ]
    }
   ],
   "source": [
    "# AdaBoostClassifier\n",
    "adaboost = ensemble.AdaBoostClassifier(n_estimators=200,learning_rate= 1)\n",
    "\n",
    "ada_score,ada_pred = clf.kfold_cw(adaboost,final_train,df_label,final_test,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(ada_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.generate_submission(gdb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_scores1 = pd.Series(adaboost.feature_importances_, index=final_train.columns).sort_values(ascending=False)\n",
    "# list(dict(feature_scores1[feature_scores1 > 0.009]))\n",
    "\n",
    "# final_train = final_train[list(dict(feature_scores1[feature_scores1 > 0.009]))]\n",
    "# final_train\n",
    "\n",
    "# feature_scores = pd.Series(adaboost.feature_importances_, index=final_test.columns).sort_values(ascending=False)\n",
    "# list(dict(feature_scores[feature_scores > 0.009]))\n",
    "\n",
    "# final_test = final_test[list(dict(feature_scores[feature_scores > 0.009]))]\n",
    "# final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  [0.85562926 0.85995946 0.86418502]\n",
      "fitting now\n",
      "final prediction\n",
      "acc:  0.8599245803395973\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "rdmforest = ensemble.RandomForestClassifier(n_estimators=200,min_samples_split=2,min_samples_leaf=4,max_depth=10,bootstrap=True)\n",
    "\n",
    "rdfst_score,rdfst_pred = clf.kfold_cw(rdmforest,final_train,df_label,final_test,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  [0.84065091 0.85534398 0.84305897 0.84459459 0.8470516  0.84643735\n",
      " 0.84981572 0.85135135 0.86087224 0.84920147]\n",
      "fitting now\n",
      "final prediction\n",
      "acc:  0.8488378178468752\n"
     ]
    }
   ],
   "source": [
    "rdmforest = ensemble.RandomForestClassifier(n_estimators=200,bootstrap=True)\n",
    "\n",
    "rdfst_score1,rdfst_pred1 = clf.kfold_cw(rdmforest,final_train,df_label,final_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  [0.83591303 0.83480744 0.84400627]\n",
      "fitting now\n",
      "final prediction\n",
      "acc:  0.8382422457547309\n"
     ]
    }
   ],
   "source": [
    "# extra tree classifier\n",
    "extratree = ensemble.ExtraTreesClassifier(n_estimators=100, max_depth=None,min_samples_split=2, random_state=0)\n",
    "extra_score, extra_pred = clf.kfold_cw(extratree,final_train,df_label,final_test,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "# NB = naive_bayes.GaussianNB()\n",
    "# nb_score,nb_pred = clf.kfold_cw(NB,final_train,df_label,final_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  [0.83342547 0.8378478  0.8417949 ]\n",
      "fitting now\n",
      "final prediction\n",
      "acc:  0.8376893862445618\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto')\n",
    "svc_score,svc_pred = clf.kfold_cw(svc,final_train,df_label,final_test,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.generate_submission(gdb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "decisiontree = tree.DecisionTreeClassifier(max_depth=None,min_samples_split=2,random_state=0)\n",
    "tree_score,tree_pred = clf.kfold_cw(decisiontree,final_train,df_label,final_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "# lasso = linear_model.Lasso(alpha=0.7)\n",
    "# lasso_score, lasso_pred = clf.kfold_cw(lasso,final_train,df_label,final_test,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = ensemble.BaggingClassifier(n_estimators=10,base_estimator=SVC(gamma='auto'),random_state=0)\n",
    "score,pred = clf.kfold_cw(bagging,final_train,df_label,final_test,3)\n",
    "\n",
    "# accuracy of [0.82725263 0.83185922 0.83165945]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With normalization and 3 kfold cv, the random forest with n_estimators=1600,min_samples_split=2,min_samples_leaf=4,max_depth=10,bootstrap=True parameters \n",
    "\n",
    "gave this accuracy scores\n",
    "\n",
    "[0.83618942 0.83812419 0.84059707]\n",
    "\n",
    "And with minmaxscaler and svr model ```best_svr = SVR(kernel='rbf',gamma='auto') ```, we got\n",
    "\n",
    "[0.85185185 0.85756403 0.86289505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ada_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(extra_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rdfst_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(gdb_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/411290/how-to-use-a-cross-validated-model-for-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.generate_submission(gradient.predict(final_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "pred = []\n",
    "step_size = [0.1,0.3,0.5,0.6,0.7,0.8,0.9,1,1.5,1.8]\n",
    "actual_pred = []\n",
    "for n,s in zip(random_grid['n_estimators'],step_size):\n",
    "    adaboost = ensemble.AdaBoostClassifier(n_estimators=n,learning_rate=s)\n",
    "\n",
    "    ada_score,ada_pred = clf.kfold_cw(adaboost,final_train,df_label,final_test,3)\n",
    "    score.append(ada_score)\n",
    "    pred.append(np.unique(ada_pred,return_counts=True))\n",
    "    actual_pred.append(ada_pred)\n",
    "    print(ada_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = pd.DataFrame()\n",
    "data_info['n_estimators'] = random_grid['n_estimators']\n",
    "data_info['step_size'] = step_size\n",
    "data_info['score'] = score\n",
    "data_info['pred_count'] = pred\n",
    "data_info['actual_pred'] = actual_pred\n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info[data_info.score == data_info['score'].max()]mis_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = []\n",
    "pred1 = []\n",
    "step_size = [0.1,0.3,0.5,0.6,0.7,0.8,0.9,1,1.5,1.8]\n",
    "for n,s in zip(random_grid['n_estimators'],step_size):\n",
    "    gradient = ensemble.GradientBoostingClassifier(n_estimators=n,learning_rate=s)\n",
    "    gdb_score,gdb_pred = clf.kfold_cw(gradient,final_train,df_label,final_test,3)\n",
    "    print(gdb_score)\n",
    "    score1.append(gdb_score)\n",
    "    pred1.append(np.unique(gdb_pred,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "\n",
    "# # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# rf = RandomForestRegressor()\n",
    "# # Random search of parameters, using 3 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(final_train, df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
